{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01d9c75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt\n",
    "import utm\n",
    "import matplotlib.ticker as ticker\n",
    "from itertools import groupby\n",
    "import os, os.path\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "import plotly.express as px\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import sys\n",
    "import csv, io\n",
    "#from eucledian_dist import eucledian_dist\n",
    "from computeDistanceMatrix import computeDistanceMatrix\n",
    "from findKNN import findKNN\n",
    "from sharedNearest import sharedNearest, countIntersection\n",
    "from density import density\n",
    "from identifyCorePoints import identifyCorePoints\n",
    "from core import core\n",
    "from findCoreNeighbors import findCoreNeighbors\n",
    "from expandCluster import expandCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "652ec926",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy(freq):\n",
    "    Probabilities = []\n",
    "    sum_of_all_freq=0\n",
    "    for ele in range(0, len(freq)):\n",
    "        sum_of_all_freq = sum_of_all_freq + freq[ele]\n",
    "    max_size = len(freq)\n",
    "    for i in range(max_size):\n",
    "      Prob = freq[i]/sum_of_all_freq\n",
    "      Probabilities.append(Prob)\n",
    "    entropy_of_loc = -np.sum(Probabilities*np.log2(Probabilities))\n",
    "    return entropy_of_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3956c881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.425271e+06</td>\n",
       "      <td>442863.182727</td>\n",
       "      <td>10.737222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.425268e+06</td>\n",
       "      <td>442863.161991</td>\n",
       "      <td>10.738056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.425269e+06</td>\n",
       "      <td>442861.755576</td>\n",
       "      <td>10.738889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.425267e+06</td>\n",
       "      <td>442859.312397</td>\n",
       "      <td>10.739722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.425232e+06</td>\n",
       "      <td>442841.971758</td>\n",
       "      <td>10.752778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Latitude      Longitude       Time\n",
       "0  4.425271e+06  442863.182727  10.737222\n",
       "1  4.425268e+06  442863.161991  10.738056\n",
       "2  4.425269e+06  442861.755576  10.738889\n",
       "3  4.425267e+06  442859.312397  10.739722\n",
       "4  4.425232e+06  442841.971758  10.752778"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the data set\n",
    "df = pd.read_csv('User_011_latlontime.csv', encoding='utf-8')\n",
    "df = df.iloc[: , 1:]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62615862",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = df[['Latitude', 'Longitude']].to_numpy()\n",
    "lats_xy = coords[:,0]\n",
    "longs_xy = coords[:,1]\n",
    "user_time = df['Time'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28336c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding minimum and maximum points from user data\n",
    "lats_index_min = np.argmin(lats_xy)\n",
    "lats_index_max = np.argmax(lats_xy)\n",
    "longs_index_min = np.argmin(longs_xy)\n",
    "longs_index_max = np.argmax(longs_xy)\n",
    "\n",
    "#Defining the boundary points\n",
    "p1_xy = (longs_xy[longs_index_min],lats_xy[lats_index_min])\n",
    "p2_xy = (longs_xy[longs_index_min],lats_xy[lats_index_max])\n",
    "p3_xy = (longs_xy[longs_index_max],lats_xy[lats_index_min])\n",
    "p4_xy = (longs_xy[longs_index_max],lats_xy[lats_index_max])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35028a9",
   "metadata": {},
   "source": [
    "### Mapping of points in a 3D space-time grid and calculate entropy after addition of each new point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8778327",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.linspace((longs_xy[longs_index_min]),(longs_xy[longs_index_max]),10)\n",
    "Y = np.linspace((lats_xy[lats_index_min]),(lats_xy[lats_index_max]), 10)\n",
    "Z = np.linspace(0,24,24)\n",
    "d=1\n",
    "grids_index = np.zeros((10,10,24))\n",
    "point_count_inside_grid = np.zeros((1,2400))\n",
    "counting = np.zeros((1,2400))\n",
    "found_grid_index = []\n",
    "allover_entropy=[]\n",
    "\n",
    "for t in range(24):\n",
    "    for yax in range(10):\n",
    "        for xax in range(10):\n",
    "            grids_index[xax,yax,t]=d\n",
    "            d=d+1\n",
    "            \n",
    "\n",
    "for user in range(len(coords)):\n",
    "    p_lat = lats_xy[user]\n",
    "    p_long = longs_xy[user]\n",
    "    p_time = user_time[user]\n",
    "    for t in range(24):\n",
    "        if ((p_time>Z[t] and p_time<Z[t+1]) or (p_time==Z[t]) or (p_time==0)):\n",
    "            for yax in range(10):\n",
    "                if (((p_lat>Y[yax]) and (p_lat<Y[yax+1])) or (p_lat==Y[yax])):\n",
    "                    for xax in range(10):\n",
    "                        if (((p_long>X[xax]) and (p_long<X[xax+1])) or (p_long==X[xax])):\n",
    "                            found_at = grids_index[xax,yax,t]\n",
    "                            found_grid_index.append(int(found_at))\n",
    "    frequency_of_grids = [len(list(group)) for key, group in groupby(found_grid_index)]\n",
    "    current_entropy = calculate_entropy(frequency_of_grids)\n",
    "    allover_entropy.append(current_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414dc6bc",
   "metadata": {},
   "source": [
    "### Normalizing the data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae30cb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "model=scaler.fit(df)\n",
    "scaled_data=model.transform(df)\n",
    "normalized_data = pd.DataFrame(scaled_data, columns = ['Latitude','Longitude','Time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f75d1a6",
   "metadata": {},
   "source": [
    "### Clustering of raw normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "200bfa91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the Pre-process the data\n",
    "latitude = normalized_data['Latitude'].to_numpy()\n",
    "longitude = normalized_data['Longitude'].to_numpy()\n",
    "time_user = normalized_data['Time'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d62525e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    k=30\n",
    "    minPoints=20\n",
    "    eps = 3\n",
    "    #To find spatial-temporal distance matrix\n",
    "    distanceMatrix = computeDistanceMatrix(latitude, longitude, time_user)\n",
    "    # To find K-nearest neighbors\n",
    "    similarityMatrix = findKNN(distanceMatrix, k)\n",
    "    count = len(latitude)\n",
    "    sharedNearestNeighbor = sharedNearest(count, k, similarityMatrix)\n",
    "    SNN_density = [None for i in range(len(sharedNearestNeighbor))]\n",
    "    for temp1 in range(len(sharedNearestNeighbor)):\n",
    "        SNN_density[temp1] = density(sharedNearestNeighbor[temp1],eps, count)\n",
    "    coreOrNot = [None for i in range(len(SNN_density))]\n",
    "\n",
    "    for temp1 in range(len(SNN_density)):\n",
    "        coreOrNot[temp1]=identifyCorePoints(SNN_density[temp1], minPoints)\n",
    "    corePoints1 = []\n",
    "    SNN_density2 = list(zip(SNN_density, [i for i in range(len(SNN_density))]))\n",
    "    for temp1 in range(len(SNN_density2)):\n",
    "        corePoints1.append(core(SNN_density2[temp1][0],SNN_density2[temp1][1], minPoints))\n",
    "    corePoints = [x for x in corePoints1 if x!=None]\n",
    "    visited=[]#list to store points visited\n",
    "    labels=[0 for i in range(count)]\n",
    "    neighborCore=[]#neighborss of core points\n",
    "    c=0 \n",
    "\n",
    "    for temp1 in range(0, len(corePoints)):\n",
    "        p = corePoints[temp1]\n",
    "        if p in visited:\n",
    "            continue\n",
    "        visited.append(p)\n",
    "        c=c+1\n",
    "        labels[p]=c\n",
    "        neighborCore=findCoreNeighbors(p, corePoints, sharedNearestNeighbor, eps)\n",
    "        labels = expandCluster(labels, neighborCore, corePoints, c, sharedNearestNeighbor, eps, visited)\n",
    "    ## Compute final cluster labels\n",
    "    #All points that are not within a radius of Eps of a core point are discarded (noise)\n",
    "    #Assign all non-noise, non-core points to their nearest core point\n",
    "\n",
    "    for temp1 in range(count):\n",
    "        notNoise=False\n",
    "        maxSim=sys.maxsize\n",
    "        bestCore=-1\n",
    "        sim=None\n",
    "        if(coreOrNot[temp1]):\n",
    "            continue\n",
    "        for temp2 in range(len(corePoints)):\n",
    "            p=corePoints[temp2]\n",
    "            #sharedNearestNeighbor contains count of shared neighbors between points\n",
    "           # sim gives the similarity  between core point and the other point.\n",
    "            sim = sharedNearestNeighbor[temp1][p]\n",
    "            # if sim is greater than eps--> the point is not a noise\n",
    "            if(sim>=eps):\n",
    "                notNoise=True\n",
    "             # if sim is less than eps--> the point is  a noise point assign cluster index 0 to it\n",
    "            else:\n",
    "                labels[temp1]=0\n",
    "                break\n",
    "            #Here we attempt to see to which core point does the non-core point has maximum similarity\n",
    "            if(sim>maxSim):\n",
    "                maxSim=sim\n",
    "                bestCore=p\n",
    "            #End of inner for loop\n",
    "        #for each non-core point assign the index of core point with which the point has maximum similarity\n",
    "        if(notNoise):\n",
    "            labels[temp1]=labels[bestCore]\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603dedef",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    labels=main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab13e936",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot=normalized_data\n",
    "to_plot.loc[:,'Label']=labels\n",
    "noise_points = to_plot[to_plot.Label==0]\n",
    "to_plot=to_plot[to_plot.Label!=0]\n",
    "print('Number of clusters are: ',len(np.unique(labels)))\n",
    "\n",
    "#Plot clustered points\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = plt.axes(projection =\"3d\")\n",
    "scatter = ax.scatter3D(to_plot['Longitude'], to_plot['Latitude'], to_plot['Time'],c=to_plot['Label'],alpha=0.9)\n",
    "ax.scatter3D(noise_points['Longitude'], noise_points['Latitude'], noise_points['Time'],color='black',alpha=0.9, label='Noise')\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "ax.set_zlim(0,1)\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "ax.set_zlabel('Time')\n",
    "first_legend = ax.legend(*scatter.legend_elements(),loc = \"upper right\", title=\"Clusters\", ncol=4, frameon=True)\n",
    "plt.gca().add_artist(first_legend)\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a50b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing \"the Silhouette Score\"\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(normalized_data, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49384c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import davies_bouldin_score\n",
    "print(davies_bouldin_score(normalized_data, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7ecaea",
   "metadata": {},
   "source": [
    "### Find points that can be transmitted directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735508b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_limit = (1/100)*len(coords) \n",
    "new_data = []\n",
    "scaled_points = []\n",
    "for temp in range(len(allover_entropy)-1):\n",
    "    if allover_entropy[temp] != 0:\n",
    "        percent_change = ((allover_entropy[temp+1]-allover_entropy[temp])/allover_entropy[temp])*100\n",
    "        if percent_change > 0:\n",
    "            new_data.append(df.iloc[temp].tolist())\n",
    "            scaled_points.append(scaled_data[temp].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f3b074",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame(new_data, columns = ['Latitude','Longitude','Time'])\n",
    "normalized_data = pd.DataFrame(scaled_points, columns = ['Latitude','Longitude','Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0830300b",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = new_df[['Latitude', 'Longitude']].to_numpy()\n",
    "lats_xy = coords[:,0]\n",
    "longs_xy = coords[:,1]\n",
    "user_time = new_df['Time'].values.tolist()\n",
    "point_count_inside_grid = np.zeros((1,2400))\n",
    "counting = np.zeros((1,2400))\n",
    "found_grid_index = []\n",
    "d=1\n",
    "new_points = []\n",
    "new_scaled_points = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dff25ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store maximum of 1% points in a grid\n",
    "for user in range(len(coords)):\n",
    "    p_lat = lats_xy[user]\n",
    "    p_long = longs_xy[user]\n",
    "    p_time = user_time[user]\n",
    "    for t in range(24):\n",
    "        if ((p_time>Z[t] and p_time<Z[t+1]) or (p_time==Z[t]) or (p_time==0)):\n",
    "            for yax in range(10):\n",
    "                if (((p_lat>Y[yax]) and (p_lat<Y[yax+1])) or (p_lat==Y[yax])):\n",
    "                    for xax in range(10):\n",
    "                        if (((p_long>X[xax]) and (p_long<X[xax+1])) or (p_long==X[xax])):\n",
    "                            found_at = grids_index[xax,yax,t]\n",
    "                            point_count_inside_grid[:,int(found_at)] += 1\n",
    "                            if point_count_inside_grid[:,int(found_at)] <= max_limit:\n",
    "                                new_points.append(new_df.iloc[user].tolist())\n",
    "                                new_scaled_points.append(normalized_data.iloc[user].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798b60bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame(new_points, columns = ['Latitude','Longitude','Time'])\n",
    "normalized_data = pd.DataFrame(new_scaled_points, columns = ['Latitude','Longitude','Time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8e404c",
   "metadata": {},
   "source": [
    "### Plot directly transmitted normalized data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b121408f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = plt.axes(projection =\"3d\")\n",
    "scatter = ax.scatter3D(normalized_data['Longitude'], normalized_data['Latitude'], normalized_data['Time'],color='blue',alpha=0.9)\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "ax.set_zlim(0,1)\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "ax.set_zlabel('Time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85057f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the Pre-process the data\n",
    "latitude = normalized_data['Latitude'].to_numpy()\n",
    "longitude = normalized_data['Longitude'].to_numpy()\n",
    "time_user = normalized_data['Time'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622edfe7",
   "metadata": {},
   "source": [
    "### Clustering of directly transmitted data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9741034",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    labels=main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2950d38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot=normalized_data\n",
    "to_plot.loc[:,'Label']=labels\n",
    "noise_points = to_plot[to_plot.Label==0]\n",
    "to_plot=to_plot[to_plot.Label!=0]\n",
    "print('Number of clusters are: ',len(np.unique(labels)))\n",
    "\n",
    "#Plot clustered points\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = plt.axes(projection =\"3d\")\n",
    "scatter = ax.scatter3D(to_plot['Longitude'], to_plot['Latitude'], to_plot['Time'],c=to_plot['Label'],alpha=0.9)\n",
    "ax.scatter3D(noise_points['Longitude'], noise_points['Latitude'], noise_points['Time'],color='black',alpha=0.9, label='Noise')\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "ax.set_zlim(0,1)\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "ax.set_zlabel('Time')\n",
    "first_legend = ax.legend(*scatter.legend_elements(),loc = \"upper right\", title=\"Clusters\", ncol=4, frameon=True)\n",
    "plt.gca().add_artist(first_legend)\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304d7240",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing \"the Silhouette Score\"\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(normalized_data, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ca72fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import davies_bouldin_score\n",
    "print(davies_bouldin_score(normalized_data, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84294549",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
